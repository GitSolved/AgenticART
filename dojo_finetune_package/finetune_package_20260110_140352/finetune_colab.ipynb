{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AgenticART Dojo - Fine-Tune WhiteRabbitNeo\\n",
        "\\n",
        "This notebook fine-tunes WhiteRabbitNeo on ADB command data collected from the Dojo.\\n",
        "\\n",
        "**Requirements:** T4 GPU (free tier) or better"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies\\n",
        "!pip install -q unsloth\\n",
        "!pip install -q --no-deps trl peft accelerate bitsandbytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Upload your training data\\n",
        "from google.colab import files\\n",
        "uploaded = files.upload()  # Upload training_data.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\\n",
        "from datasets import Dataset\\n",
        "from unsloth import FastLanguageModel\\n",
        "from trl import SFTTrainer\\n",
        "from transformers import TrainingArguments\\n",
        "\\n",
        "# Load training data\\n",
        "with open('training_data.json') as f:\\n",
        "    training_data = json.load(f)\\n",
        "print(f'Loaded {len(training_data)} examples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Format data\\n",
        "def format_alpaca(example):\\n",
        "    if example.get('input'):\\n",
        "        text = f\\\"\\\"\\\"### Instruction:\\n",
        "{example['instruction']}\\n",
        "\\n",
        "### Input:\\n",
        "{example['input']}\\n",
        "\\n",
        "### Response:\\n",
        "{example['output']}\\\"\\\"\\\"\\n",
        "    else:\\n",
        "        text = f\\\"\\\"\\\"### Instruction:\\n",
        "{example['instruction']}\\n",
        "\\n",
        "### Response:\\n",
        "{example['output']}\\\"\\\"\\\"\\n",
        "    return {'text': text}\\n",
        "\\n",
        "dataset = Dataset.from_list(training_data)\\n",
        "dataset = dataset.map(format_alpaca)\\n",
        "print(f'Formatted {len(dataset)} examples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load model\\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\\n",
        "    model_name='WhiteRabbitNeo/WhiteRabbitNeo-2.5-Qwen-2.5-Coder-7B',\\n",
        "    max_seq_length=2048,\\n",
        "    load_in_4bit=True,\\n",
        ")\\n",
        "\\n",
        "# Add LoRA\\n",
        "model = FastLanguageModel.get_peft_model(\\n",
        "    model,\\n",
        "    r=16,\\n",
        "    lora_alpha=32,\\n",
        "    lora_dropout=0.05,\\n",
        "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\\n",
        "    use_gradient_checkpointing='unsloth',\\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training\\n",
        "trainer = SFTTrainer(\\n",
        "    model=model,\\n",
        "    tokenizer=tokenizer,\\n",
        "    train_dataset=dataset,\\n",
        "    args=TrainingArguments(\\n",
        "        output_dir='./output',\\n",
        "        num_train_epochs=3,\\n",
        "        per_device_train_batch_size=4,\\n",
        "        gradient_accumulation_steps=4,\\n",
        "        learning_rate=1e-05,\\n",
        "        warmup_ratio=0.03,\\n",
        "        fp16=True,\\n",
        "        logging_steps=10,\\n",
        "        optim='adamw_8bit',\\n",
        "    ),\\n",
        "    dataset_text_field='text',\\n",
        "    max_seq_length=2048,\\n",
        "    packing=True,\\n",
        ")\\n",
        "\\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save as GGUF for Ollama\\n",
        "model.save_pretrained_gguf(\\n",
        "    'WhiteRabbitNeo-ADB-Dojo',\\n",
        "    tokenizer,\\n",
        "    quantization_method='q4_k_m',\\n",
        ")\\n",
        "\\n",
        "# Download the model\\n",
        "from google.colab import files\\n",
        "files.download('WhiteRabbitNeo-ADB-Dojo-unsloth.Q4_K_M.gguf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import to Ollama\\n",
        "\\n",
        "After downloading, create a Modelfile:\\n",
        "```\\n",
        "FROM ./WhiteRabbitNeo-ADB-Dojo-unsloth.Q4_K_M.gguf\\n",
        "```\\n",
        "\\n",
        "Then run:\\n",
        "```bash\\n",
        "ollama create whiterabbit-adb-dojo -f Modelfile\\n",
        "```"
      ]
    }
  ]
}