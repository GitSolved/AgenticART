# ═══════════════════════════════════════════════════════════════════════════════
# AgenticART Configuration
# Copy to .env and fill in your values
# ═══════════════════════════════════════════════════════════════════════════════

# ─── LLM Provider Selection ─────────────────────────────────────────────────
# Active provider: ollama | openai | anthropic
LLM_PROVIDER=ollama

# ─── Ollama Configuration (Recommended - Local, Free, Private) ──────────────
# Install: https://ollama.ai/download
# List models: ollama list
# Pull model: ollama pull qwen2.5-coder:32b
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=qwen2.5-coder:32b
OLLAMA_TEMPERATURE=0.7
OLLAMA_CONTEXT_LENGTH=32768

# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ MODEL RECOMMENDATIONS (2024-2025 benchmarks)                                │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ TIER 1 - Best for Code/Exploit Generation (requires 20-48GB RAM):           │
# │   qwen2.5-coder:32b    - RECOMMENDED: 84.1 HumanEval, 128K context, 19GB    │
# │   deepseek-coder-v2    - MoE architecture, strong reasoning                 │
# │   codestral:22b        - Mistral's code model, excellent scripts            │
# │                                                                             │
# │ TIER 2 - Uncensored/Security-Focused (won't refuse exploit prompts):        │
# │   dolphin3-abliterated - Abliterated (no refusals), various sizes           │
# │   dolphin-mistral:7b   - Fast, uncensored, good for iteration               │
# │                                                                             │
# │ TIER 3 - Fallback (8-16GB RAM):                                             │
# │   qwen2.5-coder:14b    - Sweet spot speed/quality, beats 33B models         │
# │   llama3.1:8b          - May REFUSE exploit generation (safety filters)     │
# │                                                                             │
# │ NOTE: Standard Llama models have RLHF safety training that blocks exploits. │
# │       Use Qwen-coder or Dolphin-abliterated for security research.          │
# └─────────────────────────────────────────────────────────────────────────────┘

# ─── NVD API Configuration (Live CVE Data) ─────────────────────────────────
# Get free API key: https://nvd.nist.gov/developers/request-an-api-key
# Without key: 5 requests/30 seconds | With key: 50 requests/30 seconds
NVD_API_KEY=

# ─── OpenAI Configuration (Optional) ────────────────────────────────────────
OPENAI_API_KEY=sk-your-key-here
OPENAI_MODEL=gpt-4-turbo-preview
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=4096

# ─── Anthropic Configuration (RECOMMENDED for best results) ────────────────
# Claude models exhibit superior penetration testing capabilities (2024 research)
# PentestGPT now uses Claude as primary model
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-sonnet-4-20250514

# ─── PentestGPT Integration ──────────────────────────────────────────────────
PENTESTGPT_MODE=api  # api | local | bridge
PENTESTGPT_ENDPOINT=

# ─── Android Emulator (Genymotion) ───────────────────────────────────────────
# macOS: /Applications/Genymotion.app
# Linux: /opt/genymotion or ~/genymotion
# Windows: C:\Program Files\Genymotion
GENYMOTION_PATH=
GENYMOTION_ADB_PATH=adb
EMULATOR_DEVICE=
EMULATOR_IP=192.168.56.101
EMULATOR_PORT=5555

# ─── Execution Settings ──────────────────────────────────────────────────────
AUTO_EXECUTE_SCRIPTS=false  # Safety: require confirmation before running
SCRIPT_TIMEOUT=300          # Seconds before script timeout
LOG_LEVEL=INFO              # DEBUG | INFO | WARNING | ERROR

# ─── Web Application ─────────────────────────────────────────────────────────
WEBAPP_HOST=127.0.0.1
WEBAPP_PORT=8501
WEBAPP_DEBUG=true
