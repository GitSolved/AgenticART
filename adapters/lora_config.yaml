# LoRA configuration for AgenticART adapters
# Used with: mlx_lm.lora -c lora_config.yaml

# LoRA specific settings
lora_parameters:
  rank: 64
  alpha: 128
  dropout: 0.05
  scale: 2.0

# Target modules for Qwen 2.5
lora_layers:
  - attention.q_proj
  - attention.k_proj
  - attention.v_proj
  - attention.o_proj
  - mlp.gate_proj
  - mlp.up_proj
  - mlp.down_proj
